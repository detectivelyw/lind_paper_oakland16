\section{Introduction}
\label{sec.introduction}

To run multiple applications on a computer, it is critical to securely
manage access to the underlying hardware. In modern computer systems,
either a hypervisor or an operating system (OS) kernel performs this
important function. Unfortunately, code within an operating system kernel
may contain flaws and vulnerabilities. If the kernel is successfully
attacked by malicious parties, these flaws can provide the attacker
unrestricted access to the system. One critical flaw, discovered in the
Linux kernel in the futex subsystem call can allow an attacker to gain ring
0 control via the futex syscall, and potentially execute arbitrary code
with kernel mode privileges~\cite{CVE-2014-3153}. \cappos{Possibly omit
this sentence.}

There is a diverse set of defensive technologies to combat this issue,
including OS virtualization, system call filtering, and library OSes.
Common security wisdom is that by running software within a virtual
machine, you are preventing the attacker from exploiting flaws in the
underlying kernel.  However, as we will show later, using virtualization
does not block about one third of vulnerabilities on the Linux kernel.  As
such, even with these technologies in place, applications running in a
virtual machine still pose a substantial risk.

In this paper, we begin by developing a metric that helps identify where
within the kernel these vulnerabilities are likely to be located. We
replicated 35 vulnerabilities bugs in the Linux kernel version 3.14.1 (???)
and analyzed the lines of the kernel where those bugs occurred.  Our
analysis shows that kernel paths used by popular applications contain fewer
security bugs, and therefore, can be exposed with reduced risk. 

While understanding the potential risk in kernel code is important, it is
insufficient to build a secure virtualization system.  Virtualization
systems frequently add code to the trusted computing base which may
increase the attack surface.  For example, LXC and Graphene are effectively
kernel patches to Linux and increase the kernel.  As such a vulnerability
in their codebase would be as much of a security risk as an flaw elsewhere
in the kernel.  However, clearly this functionality to implement these
corner cases must exist somewhere or else applications will not run. For
example, a vulnerability in VMWare caused by buffer overflows in the VIX
API could allow local users to execute arbitrary code in the host
OS~\cite{CVE-2008-2100}.  \cappos{Yiwen need to look if this is relevant.
It just seems like a VMWare escape rather than one that allows access to
the kernel.}

To address this issue, we propose a new design paradigm
“safely-reimplement” for virtualization systems that leverages our metric.
To prevent the kernel vulnerabilities from being exploited, we build a
sandboxed environment that uses kernel paths used by popular applications.
We then implement a POSIX interface inside of this safe sandbox, hence the
name safely-reimplement. The safe-reimplementation of POSIX causes the
complex code to be trapped within the sandbox, making it unable to trigger
kernel paths that are not used by popular applications.

Lastly, we used the design paradigm to develop a virtualization system
Lind.  Lind uses Google Native client for software fault isolation (memory
safety of the application) and the Repy sandbox to contain the POSIX
implementation and to provide access to the kernel.  This implementation
helped us to understand key issues about which kernel paths are hardest to
safely-reimplement and provided key insights about what kernel paths system
designers should give the highest degree of scrutiny.

The effectiveness of Lind was evaluated in two-steps. First, we captured
kernel traces from user programs run in Lind and four other virtualization
systems, and compared their kernel traces. Second, we examined historical
kernel bug reports to verify which trace was more likely to trigger bugs.
Results showed that applications run in Lind are the least likely to
trigger kernel bugs. Our implementation of Lind only triggered one (2.9\%)
of the 35 kernel vulnerabilities, while virtualization systems built
without our metric triggered more vulnerabilities (23-40\%). This
suggests that our metric can help effectively design and build more secure
virtualization systems.

The main contributions of this paper are as follows: \cappos{revise}
\begin{itemize}
\item We proposed a novel metric for quantitatively measuring and evaluating the
security of privileged code, such as in an OS kernel. Our metric examinesd
the safety of the kernel trace, at the lines-of-code level, generated by
running user applications and producing design recommendations.
\item Using our metric, we have substantiated our key hypothesis that commonly
used kernel paths contain fewer bugs. 
\item We created a novel secure “safely-reimplement” design by examining and
leveraging our metric and key hypothesis that commonly used kernel paths
contain fewer bugs. Our design reimplements risky system calls inside a
sandbox that only uses safe kernel paths. 
\item With this new design, we implemented a sandbox security system, Lind, that
provides a secure environment for applications and strong protection for
the kernel.
\item Results showed that the implementation of Lind only triggered one (2.9\%) of
the kernel vulnerabilities we examined, while systems built without using
our metric triggered significantly (10x) more vulnerabilities. This
suggests that our metric can help design and build virtualization systems
with greater security. 
\end{itemize}


The remainder of this paper is organized as follows. 
We discuss the motivation that drove our work and key background information 
in \S{\ref{sec.motivation-and-background}}. 
In \S{\ref{sec.metric}}, we propose our kernel coverage safety metric to solve 
the existing security problems.  A new system design using this metric is 
introduced in \S{\ref{sec.design}}. In \S{\ref{sec.implementation}}, 
we describe the implementation of our design, the sandbox security 
system Lind. Evaluation results of Lind compared to other existing 
virtualization systems are presented in \S{\ref{sec.evaluation}}. 
Limitations and future work are discussed in \S{\ref{sec.limitation}}.  
We then discuss related work in \S{\ref{sec.related_work}} and conclude
in \S{\ref{sec.conclusion}}.

